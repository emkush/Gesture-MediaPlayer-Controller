# ğŸ¬ CMPT 310 Gesture-Controlled Media Player

**Advanced AI System: Hand Gesture Recognition & Browser Media Control**

<p align="center">
  <b>CMPT 310 Â· Fall 2025 Â· SFU</b><br>
  <i>Hybrid AI system that blends motion detection and gesture recognition for video players.</i>
</p>

---

## ğŸ“‹ Overview

This project i## ğŸ™ Acknowledgments

- **CMPT 310 Course Team** for project guidance
- **Google MediaPipe** for hand tracking technology
- **TensorFlow** for machine learning framework
- **Chrome Extensions API** for web integration
- **SFU Computer Science** for educational support

---

**Ready to control your media with gestures? Run `python run.py` to get started!** ğŸ¬âœ‹

### ğŸ¯ Project Goals Achieved

This combined system demonstrates:
- âœ… **Multi-modal AI**: Static + Dynamic gesture recognition
- âœ… **Real-world Application**: Browser media control
- âœ… **Scalable Architecture**: Extensible to new gestures/sites
- âœ… **User-friendly Interface**: Simple installation and usage
- âœ… **Educational Value**: Comprehensive AI/ML implementation

**CMPT 310 Fall 2025 - Advanced AI Techniques in Action!** ğŸš€prehensive hand gesture recognition system** that combines:
- ğŸ¤š **Static gesture recognition** via **MediaPipe Model Maker** (`.task` model)
- ğŸ‘‹ **Dynamic motion detection** using real-time hand landmark tracking
- ğŸŒ **Browser integration** through Chrome extension
- ğŸ¥ **Real-time media control** for YouTube & Netflix

The system merges two powerful approaches:
1. **Gesture-MediaPlayer-Controller-main**: Core gesture recognition models and training
2. **Project**: Web extension implementation and MediaPipe integration

## âœ¨ Features

|  Type | Example |  Description |
|----------|-------------|----------------|
| ğŸ– **Static Gestures** | ğŸ‘ Play Â· âœ‹ Pause Â· ğŸ‘Š Stop | Trained using **MediaPipe Model Maker** (`.task` model) |
| ğŸ‘‹ **Dynamic Gestures** | Wave Left â†’ â®ï¸ Previous Â· Wave Right â†’ â­ï¸ Next | Real-time **motion tracking** of wrist x-direction |
| ğŸ”Š **Custom Gestures** | Swipe Up/Down for Volume Â· Double Swipe Left = Skip 30s | Hand motion logic with temporal tracking |
| ğŸ¤Ÿ **ASL Mode** | A, B, C... | Recognize ASL letters via **ASL Alphabet Dataset** |
| ğŸŒ **Browser Control** | YouTube & Netflix integration | Chrome extension with seamless media control |

## ğŸš€ Quick Start

```bash
# Single command to get started
python run.py
```

Choose **Option 1** for MediaPipe gesture control using your `.task` file!

## âœ¨ Features

- **ğŸ¤– MediaPipe Task Recognition**: Uses your custom `gesture_recognizer.task` for high accuracy
- **ğŸ¯ AI-Powered Detection**: Custom trained model for palm gesture recognition  
- **ğŸ“¹ Real-time Processing**: Fast MediaPipe-based hand tracking
- **ğŸŒ Browser Integration**: Chrome extension for seamless control
- **ğŸ¥ Multi-platform**: Works with YouTube & Netflix

## ğŸ“‹ Requirements

- Python 3.8+
- Chrome/Chromium browser
- Webcam
- macOS/Linux/Windows

## ğŸ› ï¸ Installation

### 1. Set up Python Environment

```bash
# Navigate to project directory
for example:
cd /Users/jethrohermawan/310/Project

# Activate virtual environment (already created)
source myvenv/bin/activate

# Install required packages (already done)
pip install opencv-python mediapipe tensorflow numpy scikit-learn requests
```

### 2. Install Chrome Extension

1. Open Chrome and go to `chrome://extensions/`
2. Enable "Developer mode" (toggle in top right)
3. Click "Load unpacked"
4. Select the `web-extension/` folder from this project
5. The extension should now appear in your extensions list

### 3. MediaPipe Task File
Your `gesture_recognizer.task` file is already in the `models/` folder and ready to use!

## ğŸ® Usage Options

| Option | Description | Best For |
|--------|-------------|----------|
| **1. MediaPipe Control** | ğŸ¤– Uses your `.task` file | **Recommended** - Most accurate |
| **2. AI Recognition** | ğŸ¯ Custom TensorFlow model | Alternative method |
| **3. Manual Camera** | ğŸ“¹ Press 'p' to trigger | Testing setup |
| **4. Extension Test** | ğŸ§ª No camera needed | Debugging browser integration |

## ğŸ“‹ How It Works

1. **Start launcher**: `python run.py`
2. **Choose MediaPipe option**: Option 1
3. **Open YouTube/Netflix**: Navigate to a video page
4. **Click extension**: Click the ï¿½ icon â†’ "Start Detection"  
5. **Show palm**: ğŸ–ï¸ Palm gesture pauses/plays video!

### ğŸš€ **EASIEST WAY - Use the Launcher:**

```bash
# Start here - interactive menu with all options
python run.py
```

### ğŸ“‹ **Manual Steps:**

#### **Step 1: Start Backend** (always required)
```bash
python gesture_api_server.py  # Keep this running
```

#### **Step 2: Choose Testing Method:**

**Option A: Test Extension (no camera)**
```bash
python tests/quick_test.py
```

**Option B: Manual Camera Control**  
```bash
python scripts/simple_test.py  # Press 'p' to trigger gestures
```

**Option C: Full AI Recognition**
```bash
python scripts/improved_gesture_controller.py  # Automatic palm detection
```

### ğŸ”§ **Browser Setup:**
1. Install extension: `chrome://extensions/` â†’ Load unpacked â†’ `web-extension/` folder
2. Open YouTube/Netflix VIDEO page (not homepage!)
3. Click ğŸ¬ extension â†’ "Start Detection" â†’ GREEN status

## ğŸ–ï¸ Supported Gestures

| Gesture | Action | Status |
|---------|--------|--------|
| ğŸ–ï¸ Palm | Play/Pause | âœ… Working |
| ğŸ‘ Thumbs Up | Volume Up | ğŸš§ Coming Soon |
| ğŸ‘ Thumbs Down | Volume Down | ğŸš§ Coming Soon |
| âœŒï¸ Peace Sign | Skip Forward | ğŸš§ Coming Soon |

## ğŸ—ï¸ Project Structure

```
ğŸ“¦ CMPT310-Project/
â”œâ”€â”€ ğŸš€ run.py                          # Main launcher (START HERE)
â”œâ”€â”€ ğŸ“‚ models/                         # AI Models & Training
â”‚   â”œâ”€â”€ gesture_recognizer.task        # MediaPipe trained model
â”‚   â”œâ”€â”€ gesture_recognizer.ipynb       # Training notebook
â”‚   â””â”€â”€ display_test.py               # Model testing utilities
â”œâ”€â”€ ğŸ“‚ src/                           # Core Application
â”‚   â”œâ”€â”€ mediapipe_gesture_controller.py # MediaPipe controller
â”‚   â”œâ”€â”€ gesture_recognition.py         # AI-based recognition  
â”‚   â””â”€â”€ gesture_api_server.py         # Backend server
â”œâ”€â”€ ğŸ“‚ scripts/                       # Scripts & Controllers
â”‚   â”œâ”€â”€ improved_gesture_controller.py # Enhanced gesture control
â”‚   â””â”€â”€ test.py                       # Basic testing
â”œâ”€â”€ ğŸ“‚ set_data/                      # Training Datasets
â”‚   â”œâ”€â”€ asl_alphabet/                 # ASL training data
â”‚   â””â”€â”€ Hagrid_data/                  # HaGRID gesture dataset
â”œâ”€â”€ ğŸ“‚ tests/                         # Testing Suite
â”‚   â”œâ”€â”€ test_mediapipe_task.py        # Test .task file
â”‚   â”œâ”€â”€ test_gestures.py              # Camera testing
â”‚   â””â”€â”€ test_extension.py             # Extension testing
â”œâ”€â”€ ğŸ“‚ web-extension/                 # Chrome Extension
â”‚   â”œâ”€â”€ manifest.json                 # Extension configuration
â”‚   â”œâ”€â”€ background.js                 # Background service worker
â”‚   â”œâ”€â”€ content.js                    # Content script for media sites
â”‚   â””â”€â”€ popup.html/.js                # Extension interface
â”œâ”€â”€ ğŸ“‚ docs/                          # Documentation
â”‚   â”œâ”€â”€ SETUP.md                      # Setup instructions
â”‚   â”œâ”€â”€ USAGE.md                      # Usage guidelines
â”‚   â””â”€â”€ TROUBLESHOOTING.md           # Troubleshooting guide
â”œâ”€â”€ ğŸ“‚ datasets/                      # Additional Training Data
â”‚   â”œâ”€â”€ ann_subsample/                # Annotated samples
â”‚   â”œâ”€â”€ ann_test/                     # Test annotations
â”‚   â””â”€â”€ ann_train_val/                # Training/validation data
â””â”€â”€ ğŸ“‚ mediapipe/                     # MediaPipe Integration
    â””â”€â”€ [MediaPipe source files]      # MediaPipe implementation
```

## ğŸ”§ System Architecture

1. **Gesture Recognition**: Uses MediaPipe to detect hand landmarks from webcam
2. **AI Classification**: TensorFlow model classifies gestures (palm vs others)
3. **Communication**: HTTP API server facilitates Python â†” Extension communication
4. **Media Control**: Content scripts inject media controls into web pages

### Model Training & Development

The project includes comprehensive training capabilities:
- **MediaPipe Model Maker**: For static gesture recognition
- **Custom TensorFlow Models**: For specialized gesture detection
- **ASL Integration**: For sign language recognition
- **Dynamic Gesture Detection**: Using temporal motion analysis

## ğŸ¯ Supported Websites

- âœ… **YouTube** (`youtube.com`) - Full support
- âœ… **Netflix** (`netflix.com`) - Full support
- ğŸš§ **Other sites** - Can be added by modifying `content.js`

## ğŸ› Troubleshooting

### Extension Not Responding (Most Common Issue)

**Problem:** Extension shows green status but nothing happens when showing palm

**Solution - Step by Step:**
```bash
# 1. Use the launcher for easy diagnosis
python run.py  # Choose option 4 for debugging

# 2. Or run manually:
python tests/debug_extension.py

# 3. Test extension manually:
python tests/quick_test.py
```

**Follow this checklist:**
1. âœ… API server running (`curl http://localhost:8081/status`)
2. âœ… Extension installed and enabled in `chrome://extensions/`
3. âœ… On YouTube/Netflix VIDEO page (not homepage)
4. âœ… Clicked "Start Detection" â†’ status indicator GREEN
5. âœ… Test manual gesture: `python quick_test.py`

### Camera/Gesture Recognition Issues
```bash
# Use simple camera test first
python scripts/simple_test.py

# Or use the launcher
python run.py  # Choose option 2

# Check camera permissions: System Preferences â†’ Privacy â†’ Camera
# Close other camera apps (Zoom, Teams, etc.)
```

### Extension Debug Steps
1. **Check Extension Console:**
   - Right-click extension icon â†’ "Inspect popup"
   - Look for error messages in Console tab

2. **Check Content Script:**
   - F12 on YouTube/Netflix page
   - Console should show: "Gesture-controlled media player loaded!"

3. **Manual Test:**
   - Run `python tests/quick_test.py`
   - Video should pause/play when gestures are sent

### Common Fixes
- **Refresh** the YouTube/Netflix page after starting detection
- **Restart** Chrome extension (disable/enable)
- **Check** you're on a video page with actual video content
- **Verify** extension permissions in Chrome settings

## ğŸ”§ Configuration

### Gesture Sensitivity
Edit `gesture_controller.py`:
```python
self.palm_threshold = 0.7  # Lower = more sensitive
self.gesture_cooldown = 2.0  # Seconds between gestures
```

### API Server Port
Edit `gesture_api_server.py` and `gesture_controller.py`:
```python
port = 8081  # Change if port conflicts
```

## ğŸš€ Development

### Adding New Gestures

1. **Collect Training Data**: Add gesture annotations to `datasets/`
2. **Update Model**: Modify `palm_model.py` to include new gestures
3. **Update Recognition**: Add detection logic in `gesture_controller.py`
4. **Update Extension**: Add new actions in `content.js`

### Adding New Websites

1. **Update Manifest**: Add site permissions to `manifest.json`
2. **Update Content Script**: Add selectors for new site in `content.js`
3. **Test Integration**: Verify media controls work on new site

## ğŸ“Š Performance

- **Latency**: ~200-500ms from gesture to action
- **Accuracy**: Depends on lighting and hand positioning
- **Resource Usage**: Moderate CPU usage for video processing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes and test thoroughly
4. Submit a pull request

## ğŸ“„ License

This project is for educational purposes. MediaPipe and TensorFlow are used under their respective licenses.

## ï¿½ Tips

- **Best results**: Good lighting, clear palm gesture
- **Gesture cooldown**: 2 seconds between detections  
- **Troubleshooting**: Run `python tests/test_mediapipe_task.py`
- **Multiple options**: Try different recognition methods if one doesn't work

The system now supports your MediaPipe task file for superior gesture recognition accuracy! ğŸ¯

## ï¿½ğŸ™ Acknowledgments

- Google MediaPipe for hand tracking
- TensorFlow for machine learning
- Chrome Extensions API for web integration

---

**Ready to control your media with gestures? Run `python run.py` to get started!** ğŸ¬âœ‹
